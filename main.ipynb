{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Installation**"
      ],
      "metadata": {
        "id": "iMeJ25ARQGSe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Make sure to install the necessary libraries:"
      ],
      "metadata": {
        "id": "28PR_58AQTYF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers datasets\n",
        "pip install transformers[torch] accelerate -U\n",
        "pip install PyGithub datasets"
      ],
      "metadata": {
        "id": "HO9BFixyQghq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Collecting Code Snippets**"
      ],
      "metadata": {
        "id": "b_4aO4u6P31D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from github import Github\n",
        "import re\n",
        "from datasets import Dataset\n",
        "\n",
        "# Initialize PyGithub with the GitHub token\n",
        "g = Github(\"Your Github Token\")\n",
        "\n",
        "# Specify the repository\n",
        "repo = g.get_repo(\"openai/gym\")\n",
        "\n",
        "# Function to extract Python functions from a script\n",
        "def extract_functions_from_code(code):\n",
        "    pattern = re.compile(r\"def\\s+(\\w+)\\s*\\(.*\\):\")\n",
        "    functions = pattern.findall(code)\n",
        "    return functions\n",
        "\n",
        "# Fetch Python files from the repository\n",
        "python_files = []\n",
        "contents = repo.get_contents(\"\")\n",
        "while contents:\n",
        "    file_content = contents.pop(0)\n",
        "    if file_content.type == \"dir\":\n",
        "        contents.extend(repo.get_contents(file_content.path))\n",
        "    elif file_content.path.endswith(\".py\"):\n",
        "        python_files.append(file_content)\n",
        "\n",
        "# Extract functions and create dataset\n",
        "data = {\"code\": [], \"function_name\": []}\n",
        "for file in python_files:\n",
        "    code = file.decoded_content.decode(\"utf-8\")\n",
        "    functions = extract_functions_from_code(code)\n",
        "    for function in functions:\n",
        "        data[\"code\"].append(code)\n",
        "        data[\"function_name\"].append(function)\n",
        "\n",
        "# Create a Hugging Face dataset\n",
        "dataset = Dataset.from_dict(data)\n",
        "\n",
        "# Save the dataset to disk\n",
        "dataset.save_to_disk(\"code_generation_dataset\")\n",
        "\n",
        "print(\"Dataset created and saved to disk.\")"
      ],
      "metadata": {
        "id": "nt7oQWh0QAPn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fine-Tuning the Model**"
      ],
      "metadata": {
        "id": "ePmlW0UyQsgX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_from_disk\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
        "\n",
        "# Load the tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "model = AutoModelForCausalLM.from_pretrained(\"Salesforce/codegen-350M-mono\")\n",
        "\n",
        "# Set the pad_token to eos_token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "# Load the dataset\n",
        "dataset = load_from_disk(\"code_generation_dataset\")\n",
        "\n",
        "# Split the dataset into training and test sets\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "# Preprocess the dataset\n",
        "def preprocess_function(examples):\n",
        "    return tokenizer(examples['code'], truncation=True, padding='max_length')\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Fine-tune the model\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=2,\n",
        "    num_train_epochs=1,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets['train'],\n",
        "    eval_dataset=tokenized_datasets['test']\n",
        ")\n",
        "\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "j3CcnHxaQp7E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Generating Code**"
      ],
      "metadata": {
        "id": "V6f6sLAxQ0-j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a function to generate code using the fine-tuned model\n",
        "def generate_code(prompt, max_length=100):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs['input_ids'], max_length=max_length)\n",
        "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_code\n",
        "\n",
        "# Test the model with a code generation prompt\n",
        "prompt = \"def merge_sort(arr):\"\n",
        "generated_code = generate_code(prompt)\n",
        "\n",
        "print(\"Generated Code:\")\n",
        "print(generated_code)"
      ],
      "metadata": {
        "id": "rXSJvJfnQ5Fi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}